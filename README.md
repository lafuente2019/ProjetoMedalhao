
# üß± Projeto Medalh√£o - Pipeline Delta Lake (IPCA, Boi Gordo e Insights Econ√¥micos)

## üìò Descri√ß√£o Geral

Este projeto implementa um pipeline completo no modelo **Medalh√£o (Bronze ‚Üí Silver ‚Üí Gold)** utilizando **Delta Lake** no **Databricks**.  
O objetivo √© coletar, integrar e transformar dados econ√¥micos provenientes do **Banco Central do Brasil (IPCA)** e do **Indicador Boi Gordo**, realizando c√°lculos de varia√ß√µes e consolida√ß√£o de insights.

Toda a orquestra√ß√£o √© feita dentro do **Databricks Workspace**, com versionamento, atualiza√ß√£o incremental via **MERGE (UPSERT)** e camadas Delta bem definidas.

---

## üèóÔ∏è Estrutura de Camadas

| Camada | Nome no Workspace | Descri√ß√£o |
|--------|------------------|------------|
| ü•â **Bronze** | `workspace.bronze_etl` | Armazena dados brutos coletados diretamente das APIs e arquivos CSV externos. |
| ü•à **Silver** | `workspace.silver_etl` | Cont√©m dados tratados e integrados (IPCA + Boi Gordo). |
| ü•á **Gold** | `workspace.gold_etl` | Camada final com indicadores e varia√ß√µes percentuais consolidadas. |

Cria√ß√£o dos schemas:
```sql
CREATE SCHEMA IF NOT EXISTS workspace.bronze_etl COMMENT 'Camada Bronze para armazenamento de dados brutos';
CREATE SCHEMA IF NOT EXISTS workspace.silver_etl COMMENT 'Camada Silver para armazenamento de dados tratados';
CREATE SCHEMA IF NOT EXISTS workspace.gold_etl COMMENT 'Camada Gold para armazenamento de dados prontos para uso';
```

---

## ‚öôÔ∏è Fun√ß√£o Gen√©rica: `merge_delta_table()`

A fun√ß√£o `merge_delta_table` √© usada por todas as camadas para realizar opera√ß√µes **incrementais e idempotentes** no Delta Lake.  
Agora, ela inclui uma l√≥gica inteligente de chaves de mesclagem (MERGE) conforme o tipo de tabela.

### üìú Implementa√ß√£o Atualizada

```python
from delta.tables import DeltaTable

def merge_delta_table(df_join, tabela_destino):
    """
    Realiza MERGE (upsert) de um DataFrame em uma tabela Delta Lake.

    Regras de chaves:
    -----------------
    - Para 'bronze_etl.ipca' ou 'bronze_etl.boi_gordo': usa as colunas 'data', 'valor'
    - Para demais tabelas: usa as colunas 'data', 'ipca', 'boi_gordo'

    Par√¢metros:
    -----------
    df_join : DataFrame
        DataFrame com os dados a serem inseridos/atualizados.
    tabela_destino : str
        Nome completo da tabela Delta (ex: 'bronze_etl.ipca' ou 'gold_etl.insights').

    Retorno:
    --------
    None
    """

    if df_join is not None and df_join.limit(1).count() > 0:
        total = df_join.count()
        print(f"‚úÖ Total de registros carregados: {total}")

        if spark.catalog.tableExists(tabela_destino):
            print(f"üì¶ Tabela {tabela_destino} j√° existe ‚Äî atualizando dados...")

            delta_table = DeltaTable.forName(spark, tabela_destino)

            # üîë Define a condi√ß√£o de merge conforme a tabela
            if tabela_destino in ["bronze_etl.ipca", "bronze_etl.boi_gordo"]:
                condicao_merge = "t.data = s.data AND t.valor = s.valor"
                print("üîë Usando chaves: data, valor")
            else:
                condicao_merge = (
                    "t.data = s.data "
                    "AND t.ipca = s.ipca "
                    "AND t.boi_gordo = s.boi_gordo"
                )
                print("üîë Usando chaves: data, ipca, boi_gordo")

            # üöÄ Executa o MERGE
            (
                delta_table.alias("t")
                .merge(
                    df_join.alias("s"),
                    condicao_merge
                )
                .whenMatchedUpdateAll()
                .whenNotMatchedInsertAll()
                .execute()
            )

            print(f"‚úÖ MERGE conclu√≠do com sucesso em {tabela_destino}")

        else:
            print(f"üÜï Tabela {tabela_destino} n√£o existe ‚Äî criando nova tabela...")

            df_join.write                 .format("delta")                 .partitionBy("data")                 .mode("append")                 .option("overwriteSchema", "true")                 .saveAsTable(tabela_destino)

            print(f"‚úÖ Tabela {tabela_destino} criada e dados inseridos com sucesso.")

    else:
        print("‚ö†Ô∏è Nenhum dado para atualizar.")
```

---

## ü•â Camada Bronze

### üìä 1. IPCA ‚Äî Coleta de Dados do Banco Central

```python
URL = 'https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados'
params = {'formato': 'json', 'dataInicial': '01/01/2024', 'dataFinal': datetime.now().strftime('%d/%m/%Y')}
response = requests.get(URL, params=params)
data = response.json()
df = spark.createDataFrame(data).toDF('data', 'valor')
df = df.withColumn("valor", col("valor").cast("double"))
df = df.withColumn("data_coleta", from_utc_timestamp(current_timestamp(), "America/Sao_Paulo"))
merge_delta_table(df, "workspace.bronze_etl.ipca")
```

| Coluna | Tipo | Descri√ß√£o |
|---------|------|-----------|
| `data` | date | Data de refer√™ncia do IPCA |
| `valor` | double | Valor percentual do IPCA |
| `data_coleta` | timestamp | Data/hora da coleta (fuso S√£o Paulo) |

---

### üêÇ 2. Boi Gordo ‚Äî Leitura e Grava√ß√£o de Dados CSV

Fonte: `/Volumes/workspace/bronze_etl/boigordo/BoiGordo.csv`

```python
df = (
    spark.read
    .option("header", "true")
    .option("sep", ";")
    .option("inferSchema", "true")
    .csv("/Volumes/workspace/bronze_etl/boigordo/BoiGordo.csv")
)

df = df.withColumnRenamed("Valor", "valor")
df = df.withColumn(
    "data_coleta",
    from_utc_timestamp(current_timestamp(), "America/Sao_Paulo")
)

merge_delta_table(df, "workspace.bronze_etl.boi_gordo")
```

| Coluna | Tipo | Descri√ß√£o |
|---------|------|-----------|
| `Data` | date | M√™s/Ano da cota√ß√£o |
| `valor` | double | Pre√ßo m√©dio do boi gordo |
| `data_coleta` | timestamp | Data/hora da coleta ajustada para o fuso hor√°rio de S√£o Paulo |

---

## ü•à Camada Silver ‚Äî Integra√ß√£o Econ√¥mica (IPCA + Boi Gordo)

### Objetivo

Integrar os dados do **IPCA** e do **Boi Gordo**, formatando datas, ajustando tipos e consolidando ambos em uma √∫nica tabela.

### Principais etapas
- Leitura das tabelas Bronze (`ipca`, `boi_gordo`)
- Padroniza√ß√£o de colunas e formata√ß√£o de datas (`yyyy-MM`)
- Jun√ß√£o e deduplica√ß√£o (`data_coleta`, `data`)
- Convers√£o de v√≠rgula para ponto no campo `boi_gordo`
- Grava√ß√£o incremental via `merge_delta_table`

---

## ü•á Camada Gold ‚Äî Insights Econ√¥micos (Varia√ß√£o Percentual)

### Objetivo

Gerar indicadores de **varia√ß√£o percentual mensal** do IPCA e do Boi Gordo.

- Deduplica√ß√£o por `data`
- C√°lculo da varia√ß√£o percentual via `Window.orderBy("data")`
- Grava√ß√£o incremental com `merge_delta_table`

---

## üß† Boas Pr√°ticas Implementadas

- ‚úÖ Arquitetura **Medalh√£o** (Bronze, Silver, Gold)
- ‚úÖ Fun√ß√£o gen√©rica e reutiliz√°vel `merge_delta_table`
- ‚úÖ Deduplica√ß√£o (`dropDuplicates`)
- ‚úÖ Tratamento de `NULL` e divis√£o segura (`F.coalesce`, `F.when`)
- ‚úÖ Timezone local (`America/Sao_Paulo`)
- ‚úÖ MERGE incremental (sem recriar tabelas)
- ‚úÖ Integra√ß√£o de m√∫ltiplas fontes (API + CSV)

---

## üßæ Autor

**Valter Lafuente Junior**  
üíº Data Engineer
üìÖ Projeto: *Pipeline Medalh√£o Delta Lake (Economia)*  
üìç Stack: *Databricks ‚Ä¢ PySpark ‚Ä¢ Delta Lake ‚Ä¢ GCP*

---

## üìé Estrutura Final das Tabelas

| Camada | Tabela | Descri√ß√£o |
|--------|---------|------------|
| Bronze | `workspace.bronze_etl.ipca` | Dados brutos do IPCA coletados da API |
| Bronze | `workspace.bronze_etl.boi_gordo` | Dados de cota√ß√£o do Boi Gordo importados via CSV |
| Silver | `workspace.silver_etl.economia` | Jun√ß√£o e padroniza√ß√£o de IPCA + Boi Gordo |
| Gold | `workspace.gold_etl.varicacao_ipca_boiGordo` | Indicadores de varia√ß√£o percentual |

---

## üöÄ Pr√≥ximos Passos

- Automatizar ingest√£o via **Databricks Jobs**
- Publicar camada Gold no **Power BI / Looker**
- Criar dashboard de varia√ß√£o econ√¥mica mensal
