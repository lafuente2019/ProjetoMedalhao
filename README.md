
# ğŸ§± Projeto MedalhÃ£o - Pipeline Delta Lake (IPCA, Boi Gordo e Insights EconÃ´micos)

## ğŸ“˜ DescriÃ§Ã£o Geral

Este projeto implementa um pipeline completo no modelo **MedalhÃ£o (Bronze â†’ Silver â†’ Gold)** utilizando **Delta Lake** no **Databricks**.  
O objetivo Ã© coletar, integrar e transformar dados econÃ´micos provenientes do **Banco Central do Brasil (IPCA)** e do **Indicador Boi Gordo**, realizando cÃ¡lculos de variaÃ§Ãµes e consolidaÃ§Ã£o de insights.

Toda a orquestraÃ§Ã£o Ã© feita dentro do **Databricks Workspace**, com versionamento, atualizaÃ§Ã£o incremental via **MERGE (UPSERT)** e camadas Delta bem definidas.

---

## ğŸ—ï¸ Estrutura de Camadas

| Camada | Nome no Workspace | DescriÃ§Ã£o |
|--------|------------------|------------|
| ğŸ¥‰ **Bronze** | `workspace.bronze_etl` | Armazena dados brutos coletados diretamente das APIs e arquivos CSV externos. |
| ğŸ¥ˆ **Silver** | `workspace.silver_etl` | ContÃ©m dados tratados e integrados (IPCA + Boi Gordo). |
| ğŸ¥‡ **Gold** | `workspace.gold_etl` | Camada final com indicadores e variaÃ§Ãµes percentuais consolidadas. |

CriaÃ§Ã£o dos schemas:
```sql
CREATE SCHEMA IF NOT EXISTS workspace.bronze_etl COMMENT 'Camada Bronze para armazenamento de dados brutos';
CREATE SCHEMA IF NOT EXISTS workspace.silver_etl COMMENT 'Camada Silver para armazenamento de dados tratados';
CREATE SCHEMA IF NOT EXISTS workspace.gold_etl COMMENT 'Camada Gold para armazenamento de dados prontos para uso';
```

---

## âš™ï¸ FunÃ§Ã£o GenÃ©rica: `merge_delta_table()`

A funÃ§Ã£o `merge_delta_table` Ã© usada por todas as camadas para realizar operaÃ§Ãµes **incrementais e idempotentes** no Delta Lake.  
Agora, ela inclui uma lÃ³gica inteligente de chaves de mesclagem (MERGE) conforme o tipo de tabela.

### ğŸ“œ ImplementaÃ§Ã£o Atualizada

```python
from delta.tables import DeltaTable

def merge_delta_table(df_join, tabela_destino):
    """
    Realiza MERGE (upsert) de um DataFrame em uma tabela Delta Lake.

    Regras de chaves:
    -----------------
    - Para 'bronze_etl.ipca' ou 'bronze_etl.boi_gordo': usa as colunas 'data', 'valor'
    - Para demais tabelas: usa as colunas 'data', 'ipca', 'boi_gordo'

    ParÃ¢metros:
    -----------
    df_join : DataFrame
        DataFrame com os dados a serem inseridos/atualizados.
    tabela_destino : str
        Nome completo da tabela Delta (ex: 'bronze_etl.ipca' ou 'gold_etl.insights').

    Retorno:
    --------
    None
    """

    if df_join is not None and df_join.limit(1).count() > 0:
        total = df_join.count()
        print(f"âœ… Total de registros carregados: {total}")

        if spark.catalog.tableExists(tabela_destino):
            print(f"ğŸ“¦ Tabela {tabela_destino} jÃ¡ existe â€” atualizando dados...")

            delta_table = DeltaTable.forName(spark, tabela_destino)

            # ğŸ”‘ Define a condiÃ§Ã£o de merge conforme a tabela
            if tabela_destino in ["bronze_etl.ipca", "bronze_etl.boi_gordo"]:
                condicao_merge = "t.data = s.data AND t.valor = s.valor"
                print("ğŸ”‘ Usando chaves: data, valor")
            else:
                condicao_merge = (
                    "t.data = s.data "
                    "AND t.ipca = s.ipca "
                    "AND t.boi_gordo = s.boi_gordo"
                )
                print("ğŸ”‘ Usando chaves: data, ipca, boi_gordo")

            # ğŸš€ Executa o MERGE
            (
                delta_table.alias("t")
                .merge(
                    df_join.alias("s"),
                    condicao_merge
                )
                .whenMatchedUpdateAll()
                .whenNotMatchedInsertAll()
                .execute()
            )

            print(f"âœ… MERGE concluÃ­do com sucesso em {tabela_destino}")

        else:
            print(f"ğŸ†• Tabela {tabela_destino} nÃ£o existe â€” criando nova tabela...")

            df_join.write                 .format("delta")                 .partitionBy("data")                 .mode("append")                 .option("overwriteSchema", "true")                 .saveAsTable(tabela_destino)

            print(f"âœ… Tabela {tabela_destino} criada e dados inseridos com sucesso.")

    else:
        print("âš ï¸ Nenhum dado para atualizar.")
```

---

## ğŸ¥‰ Camada Bronze

### ğŸ“Š 1. IPCA â€” Coleta de Dados do Banco Central

```python
URL = 'https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados'
params = {'formato': 'json', 'dataInicial': '01/01/2024', 'dataFinal': datetime.now().strftime('%d/%m/%Y')}
response = requests.get(URL, params=params)
data = response.json()
df = spark.createDataFrame(data).toDF('data', 'valor')
df = df.withColumn("valor", col("valor").cast("double"))
df = df.withColumn("data_coleta", from_utc_timestamp(current_timestamp(), "America/Sao_Paulo"))
merge_delta_table(df, "workspace.bronze_etl.ipca")
```

| Coluna | Tipo | DescriÃ§Ã£o |
|---------|------|-----------|
| `data` | date | Data de referÃªncia do IPCA |
| `valor` | double | Valor percentual do IPCA |
| `data_coleta` | timestamp | Data/hora da coleta (fuso SÃ£o Paulo) |

---

### ğŸ‚ 2. Boi Gordo â€” Leitura e GravaÃ§Ã£o de Dados CSV

Fonte: `/Volumes/workspace/bronze_etl/boigordo/BoiGordo.csv`

```python
df = (
    spark.read
    .option("header", "true")
    .option("sep", ";")
    .option("inferSchema", "true")
    .csv("/Volumes/workspace/bronze_etl/boigordo/BoiGordo.csv")
)

df = df.withColumnRenamed("Valor", "valor")
df = df.withColumn(
    "data_coleta",
    from_utc_timestamp(current_timestamp(), "America/Sao_Paulo")
)

merge_delta_table(df, "workspace.bronze_etl.boi_gordo")
```

| Coluna | Tipo | DescriÃ§Ã£o |
|---------|------|-----------|
| `Data` | string | MÃªs/Ano da cotaÃ§Ã£o |
| `valor` | double | PreÃ§o mÃ©dio do boi gordo |
| `data_coleta` | timestamp | Data/hora da coleta ajustada para o fuso horÃ¡rio de SÃ£o Paulo |

---

## ğŸ¥ˆ Camada Silver â€” IntegraÃ§Ã£o EconÃ´mica (IPCA + Boi Gordo)

*(mantida conforme versÃ£o anterior, com join, padronizaÃ§Ã£o de datas e deduplicaÃ§Ã£o)*

---

## ğŸ¥‡ Camada Gold â€” Insights EconÃ´micos (VariaÃ§Ã£o Percentual)

*(mantida conforme versÃ£o anterior, com cÃ¡lculo de variaÃ§Ã£o percentual mensal e MERGE final)*

---

## ğŸ” Fluxo de Dados

```mermaid
graph TD
    A[API Banco Central (IPCA)] -->|IngestÃ£o| B[workspace.bronze_etl.ipca]
    A2[CSV - Boi Gordo] -->|IngestÃ£o| B2[workspace.bronze_etl.boi_gordo]
    B & B2 -->|TransformaÃ§Ã£o + Join| C[workspace.silver_etl.economia]
    C -->|CÃ¡lculo de variaÃ§Ãµes percentuais| D[workspace.gold_etl.varicacao_ipca_boiGordo]
```

---

## ğŸ§  Boas PrÃ¡ticas Implementadas

- âœ… Arquitetura **MedalhÃ£o** (Bronze, Silver, Gold)
- âœ… FunÃ§Ã£o **inteligente** `merge_delta_table` com chaves dinÃ¢micas
- âœ… DeduplicaÃ§Ã£o (`dropDuplicates`)
- âœ… Tratamento de `NULL` e divisÃ£o segura (`F.coalesce`, `F.when`)
- âœ… Timezone local (`America/Sao_Paulo`)
- âœ… MERGE incremental (sem recriar tabelas)
- âœ… IntegraÃ§Ã£o de mÃºltiplas fontes (API + CSV)

---

## ğŸ§¾ Autor

**Valter Lafuente Junior**  
ğŸ’¼ Data Engineer 
ğŸ“… Projeto: *Pipeline MedalhÃ£o Delta Lake (Economia)*  
ğŸ“ Stack: *Databricks â€¢ PySpark â€¢ Delta Lake â€¢ GCP*
